{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/04_Classification'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 4: Classification\n",
    "\n",
    "### 4.1. Learning a classifier for the Iris Data Set – Part II\n",
    "\n",
    "In the last exercise, you have learned classification models for the Iris dataset using a train/test split. Now try learning a decision tree and evaluate it with 10-fold cross-validation. Use a pipeline to perform some preprocessing before learning or applying the decision tree classifier. For this exercise, we use the ```iris_imbalanced.csv``` dataset, so it's a good idea to include a balancing step in the preprocessing!\n",
    "\n",
    "#### 4.1.1.\tDiscretise the Iris data set into three bins. Then use the DecisionTreeClassifier with a 10-fold stratified cross validation and compute the accuracy. Afterwards plot the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "## Windows users: either add the path to graphviz' dot.exe to your PATH variable \n",
    "## OR comment in the 2 lines below (may have to change path):\n",
    "\n",
    "# import os\n",
    "# os.environ['PATH'] += ';C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin'\n",
    "\n",
    "# load the dataset\n",
    "iris = pd.read_csv(\"iris_imbalanced.csv\")\n",
    "iris_data = iris[['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']]\n",
    "iris_target = iris['Name']\n",
    "\n",
    "# define a pipeline including a discretiser, balancing, and a decision tree classifier\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "# evaluate the pipeline\n",
    "accuracy = cross_val_score(pipeline, iris_data, iris_target, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the pipeline to the dataset \n",
    "pipeline.fit(iris_data, iris_target)\n",
    "\n",
    "print(\"Fitted a decision tree with {} nodes. Cross-validated accuracy is {}%\".format(estimator.tree_.node_count, accuracy.mean() * 100))\n",
    "\n",
    "# plot the decision tree\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "\n",
    "dot_data = tree.export_graphviz(estimator, out_file=None, \n",
    "                                feature_names=iris_data.columns, \n",
    "                                class_names=iris_target.unique(),\n",
    "                               filled=True, rounded=True, special_characters=True) \n",
    "\n",
    "graph = graphviz.Source(dot_data) \n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.2.\tRemove the discretization and adjust the max_depth parameter of DecisionTreeClassifier to increase the accuracy. Does the accuracy change? Compare the complexity of the two models. Which model should be preferred according to Occam’s razor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable the discretiser\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "# re-evaluate the pipeline\n",
    "accuracy = cross_val_score(pipeline, iris_data, iris_target, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the pipeline to the dataset again\n",
    "pipeline.fit(iris_data, iris_target)\n",
    "\n",
    "print(\"Fitted a decision tree with {} nodes. Cross-validated accuracy is {}%\".format(estimator.tree_.node_count, accuracy.mean() * 100))\n",
    "\n",
    "# plot the decision tree\n",
    "dot_data = tree.export_graphviz(estimator, out_file=None, \n",
    "                                feature_names=iris_data.columns, \n",
    "                                class_names=iris_target.unique(),\n",
    "                               filled=True, rounded=True, special_characters=True) \n",
    "\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# adjust the max_depth parameter and re-evaluate\n",
    "for depth in range(1, 9):\n",
    "    #TODO: INSERT YOUR CODE HERE!\n",
    "    \n",
    "    accuracy = cross_val_score(pipeline, iris_data, iris_target, cv=10, scoring='accuracy')\n",
    "    print(\"max_depth={}: {}% accuracy\".format(depth, accuracy.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# choose a good parameter setting and re-fit the pipeline to the dataset\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "\n",
    "\n",
    "pipeline.fit(iris_data, iris_target)\n",
    "dot_data = tree.export_graphviz(estimator, out_file=None, \n",
    "                                feature_names=iris_data.columns, \n",
    "                                class_names=iris_target.unique(),\n",
    "                               filled=True, rounded=True, special_characters=True) \n",
    "\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Which model would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Who should get a bank credit?\n",
    "The German credit data set from the UCI data set library (http://archive.ics.uci.edu/ml/index.html) describes the customers of a bank with respect to whether they should get a bank credit or not. The data set is provided as credit-g.arff file in ILIAS. \n",
    "\n",
    "#### 4.2.1. Plot ROC curves for k-NN (different k values) and Decision Tree (you can use the given avg_roc function) . Which classification approach looks most promising to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "credit_arff_data, credit_arff_meta = arff.loadarff(open('credit-g.arff', 'r'))\n",
    "credit = pd.DataFrame(credit_arff_data)\n",
    "# select all columns of type object\n",
    "columns_with_binary_strings = credit.select_dtypes('object').columns.values\n",
    "\n",
    "# decode the values of these columns using utf-8\n",
    "credit[columns_with_binary_strings] = credit[columns_with_binary_strings].apply(lambda x: x.str.decode(\"utf-8\"))\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First define the preprocessing. Have a look at the class distribution and feature types and think about appropriate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# separate the target variable from the features\n",
    "credit_target = credit['class']\n",
    "credit_data = credit.drop('class', axis=1)\n",
    "\n",
    "# plot the class distribution\n",
    "#TODO: INSERT YOUR CODE HERE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# think about which features you want to re-scale, encode using one-hot encoding or encode using ordinal encoding\n",
    "# then, create a ColumnTransformer to execute this preprocessing for you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell once to define the ```avg_roc``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#define function for computing average roc for cross validation\n",
    "#see http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def avg_roc(cv, estimator, data, target, pos_label):\n",
    "    mean_fpr = np.linspace(0, 1, 100) # = [0.0, 0.01, 0.02, 0.03, ... , 0.99, 1.0]\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    \n",
    "    for train_indices, test_indices in cv.split(data, target):\n",
    "        train_data = data.iloc[train_indices]\n",
    "        train_target = target[train_indices]\n",
    "        estimator.fit(train_data, train_target)\n",
    "\n",
    "        test_data = data.iloc[test_indices]\n",
    "        test_target = target[test_indices]\n",
    "        decision_for_each_class = estimator.predict_proba(test_data)#have to use predict_proba or decision_function \n",
    "    \n",
    "        fpr, tpr, thresholds = roc_curve(test_target, decision_for_each_class[:,1], pos_label=pos_label)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0 # tprs[-1] access the last element\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "        \n",
    "        #plt.plot(fpr, tpr)# plot for each fold\n",
    "        \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0 # set the last tpr to 1\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    return mean_fpr, mean_tpr, mean_auc, std_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate different classifiers using the ```avg_roc``` function and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# define the cross validation folds\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# create the pipeline, we will set the estimator later\n",
    "pipeline = Pipeline([ ('preprocessing', preprocessor), ('estimator', None) ])\n",
    "\n",
    "# setup a figure\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random', alpha=.8) # draw diagonal\n",
    "\n",
    "# KNN\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "for n_neighbour in [2,3,4,5,6]:\n",
    "    #TODO: INSERT YOUR CODE HERE!\n",
    "    mean_fpr, mean_tpr, mean_auc, std_auc = avg_roc(cv, pipeline, credit_data, credit_target, 'good')\n",
    "    plt.plot(mean_fpr, mean_tpr, label='{}-NN (AUC: {:.3f} $\\pm$ {:.3f})'.format(n_neighbour, mean_auc, std_auc))\n",
    "    \n",
    "# Decision Tree\n",
    "#TODO: INSERT YOUR CODE HERE!\n",
    "mean_fpr, mean_tpr, mean_auc, std_auc = avg_roc(cv, pipeline, credit_data, credit_target, 'good')\n",
    "plt.plot(mean_fpr, mean_tpr, label='DecisonTree (AUC: {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc))\n",
    "\n",
    "# show the plot\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.2.\tFor the two most promising classification approaches, compute the accuracy and confusion matrix in a 10-fold CV setup. Which level of accuracy do you reach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Plot confusion matrix and classification report for the two most promising approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.3.\tWhat do the precision and recall values for the class “bad”  tell you? Try to improve the situation by increasing the number of “bad” examples in the training set (in the cross validation). How do precision and recall change if you apply this procedure? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# upsample the \"bad\" examples and evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.4.\tTo model a use case specific evaluation, compute the cost of all missclassifications. Set up your cost matrix by assuming that you will lose 1 Unit if you refuse a credit to a good customer, but that you lose 100 Units if you give a bad customer a credit. Rerun the experiments and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create the predictions here\n",
    "# prediction = ...\n",
    "\n",
    "# You can use the following code to calculate the cost\n",
    "cm = confusion_matrix(credit_target, prediction, labels=credit_target.unique())\n",
    "cost = cm[0][1] * 100 + cm[1][0] * 1\n",
    "acc = accuracy_score(credit_target, prediction)\n",
    "print(\"6-NN with accuracy of {} and cost {}\".format(acc, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.5.\tAs the creation of training data is mostly a manual task and humans tend to be fallible, training data might include noise. Simulate this behavior by using the Add Noise function and change the parameter “percentage” from 0% over 10% to 20%. Is your preferred classification approach still feasible for this situation? How does the performance of the other classifiers evolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random \n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def add_noise(raw_target, percentage):    \n",
    "    labels = unique_labels(raw_target)\n",
    "    target_with_noise = []\n",
    "    for one_target_label in raw_target:\n",
    "        if random.randint(1,100) <= percentage:\n",
    "            target_with_noise.append(next(l for l in labels if l != one_target_label))\n",
    "        else:\n",
    "            target_with_noise.append(one_target_label)\n",
    "    return target_with_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add noise to your training splits and evaluate (e.g. use manual cross validation (see intro slides) for this)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
